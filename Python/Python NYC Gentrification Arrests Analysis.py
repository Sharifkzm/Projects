# -*- coding: utf-8 -*-
"""ms6578 Python Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_DcIkHE7Sqx1UoWCUEunhg0rlcEnc1Tt

# Trends in genrtrfication and arrests in New York City

## Introduction

Hello!

In this project, I wanted to achieve two things:

1.   Examine how NYC trends in gentrification may (or may not) overlap with arrests
2.   Stretch my geospatial analysis skills in Python

I hope that you'll enjoy the results.

## Question
What is the relationship betweem changes in gentrification across a neighbourhood and the number of arrests per capita in that neighbourhood?

## Hypothesis
I would suspect that an increase in gentrification would drive up the arrests/capita in a neighbourhood as new, wealthier residents might call the police more often and there would be more stark social inequalities that could cause conflict and crime.

## Main datasets
I explain each of these in deeper detail in-turn:
*   **NYC Arrests**: https://data.cityofnewyork.us/Public-Safety/NYPD-Arrests-Data-Historic-/8h9b-rp9u
*   **Gentrification index study**: https://www-tandfonline-com.ezproxy.cul.columbia.edu/doi/full/10.1080/13658816.2021.1931873
*   **NYC Census Tracts**: https://www.nyc.gov/site/planning/data-maps/open-data/census-download-metadata.page

## Loading data: part 1

### Arrests dataset


*   **Source**: https://data.cityofnewyork.us/Public-Safety/NYPD-Arrests-Data-Historic-/8h9b-rp9u
*   **File type**: .csv
*   **Description**: This is a breakdown of every arrest in NYC by the NYPD going back to 2006 through the end of 2022.


**LOADING THIS DATA WILL TAKE ABOUT 4 MINS**. I'm sorry about that, but I reduce the size in the next stages to make sure all the subsequent analyses are fast. This may be a good opportunity to reply to that email you've been putting off...
"""

import pandas as pd
import requests

df=pd.read_csv("https://data.cityofnewyork.us/api/views/8h9b-rp9u/rows.csv?accessType=DOWNLOAD")

"""> Sorry again that that took a while... I hope it proved good motivation for replying to emails?
>
> Let's see if we can cut the data down a bit to make it more usuable. Let's examine the columns to see which ones are most useful for us.
"""

list(df.columns)

"""> We can consult the data dictionary in the source to see which ones might be more useful: https://data.cityofnewyork.us/Public-Safety/NYPD-Arrests-Data-Historic-/8h9b-rp9u
> *   Arrest key, date, and longitude/latitude will be helpful to place the arrest
> *   And descriptor variables like the description of the offence, age, sex, and race of the perpetrator will be helpful
>
> While we're at it, let's make sure the date column is in a usable format.
"""

# reducing size of dataset
reduced_df_one = df[['ARREST_KEY',
             'ARREST_DATE',
             'OFNS_DESC',
             'AGE_GROUP',
             'PERP_SEX',
             'PERP_RACE',
             'Latitude',
             'Longitude'
             ]].copy()

reduced_df_one["ARREST_DATE"] = pd.to_datetime(df["ARREST_DATE"], format="%m/%d/%Y")

reduced_df_one

"""> Great, now that we have that let's plot the results again time to see what the headline trend for arrests in NYC has been..."""

import plotly.express as px
arrests_per_year = reduced_df_one.resample("Y", on="ARREST_DATE").size().reset_index(name="count")

fig = px.scatter(
    arrests_per_year,
    x="ARREST_DATE",
    y="count",
    title="Number of arrests in NYC, 2006-2022",
    trendline="ols",
)

fig.show()

"""> Ok that's helpful to see the overall trend. Let's cut up the data into two segments that we can more easily use as benchmarks.
>
> We'll be taking 2006 and 2016 as benchmark years as that corresponds best to the Gentrifiaction study covered later on (2000-2016), and we'll drop any values that don't have latitude as we won't be able to map them.
"""

reduced_df_two = reduced_df_one[reduced_df_one["ARREST_DATE"] < '2017-01-01']

reduced_df_twotwo = reduced_df_two[reduced_df_one["ARREST_DATE"] > '2015-12-31']

reduced_df_three_2016 = reduced_df_twotwo.dropna(subset=['Latitude'])

reduced_df_three_2016

reduced_df_two_2006 = reduced_df_one[reduced_df_one["ARREST_DATE"] < '2007-01-01']

reduced_df_three_2006 = reduced_df_two_2006.dropna(subset=['Latitude'])

reduced_df_three_2006

"""> Let's pick one of the years and plot the data using the Latitude and Longitude on a basic graph. Here are some guides that helped me:
>*   https://medium.com/analytics-vidhya/plotly-for-geomaps-bb75d1de189f
>*   https://www.matecdev.com/posts/point-in-polygon.html


"""

fig = px.scatter_mapbox(reduced_df_three_2016, lat="Latitude", lon="Longitude",
                        color_discrete_sequence=["fuchsia"], zoom=8, height=300)
fig.update_layout(mapbox_style="white-bg")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""> Alrightie, the arrests data is primed and ready. Now we can move to the other datasets.

## Loading data: part 2

### Gentrification index

*   **Source**: You can access the full article via CLIO - https://www-tandfonline-com.ezproxy.cul.columbia.edu/doi/full/10.1080/13658816.2021.1931873
*   **File type**: Excel
*   **Description**: This excel workbook contains the source variables, relative change input variables and final output variables for calculating an index of gentrification for NYC census tracts, representing change from the year 2000 to 2016.

**Specifically on gentrification,** we will use the variable 'score_0.5' which is an index of gentrification created by this study which takes into account changes in several factors such as house prices, demographics (age, race, education), etc.  

As the data is in Excel format, we need to convert it to CSV. This guide helped me with that process: https://www.geeksforgeeks.org/convert-excel-to-csv-in-python/

NB. There was a KML version of the data already-mapped onto a geospatial file, but I couldn't get collab to work with KML converters so I have a workaround later on :)
"""

gent_excel=pd.read_excel('https://figshare.com/ndownloader/files/27926934')

gent_excel.to_csv ("gent_csv", index = None, header=True)

gent_df = pd.DataFrame(pd.read_csv("gent_csv"))

gent_df

"""> Lots of great data here, but we're mainly interested in a couple of things:
> *   GEOID
> *   NTA name (to do a quick spot check on whether our match has worked)
> *   'score_0.5' for the Gentrification data, defined in the data dictionary as '50 %-tile (median) of Bayesian spatially smoothed score, used as the index of gentrification'
> *   'totpop00' for the population in the tract
"""

gent_df['GEOID'] = gent_df['tractid']

gent_df['GEOID']=gent_df['GEOID'].astype(int)

gent_df_reduced = gent_df[['GEOID',
             'nta_name',
             'totpop00',
             'score_0.5'
             ]].copy()

gent_df_reduced

"""## Loading data: part 3

### NYC Census Tracts

*   **Source**: https://www.nyc.gov/site/planning/data-maps/open-data/census-download-metadata.page
*   **File type**: GeoJSON
*   **Description**: 2010 Census Blocks and Census Tracts from the US Census for New York City. These Tracts allow us to join data from the Gentrification Index and conduct spatial analyses.

As we're now moving into GIS, we need to install and import a number of additional tools before we import the GeoJSON. To be able to play with this geospatial file like a dataframe (because that's our happy place as fans of Pandas), we need to convert it to a GeoDataFrame once it's loaded.
"""

!pip install geopandas
!pip install geoplot
import geopandas as gpd
import numpy as np
from shapely.geometry import Point
import geoplot
import json

Censustracts = gpd.GeoDataFrame.from_file('https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2010_US_Census/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=pgeojson')

Censustracts

"""> OK, all is loaded now - let's get **joining!**

## Joining the data: part 1

### Gentrification and Census Tracts

We need a key between the Gentrification and Census datasets to join them on. Unfortunately, it seems that the 'GEOID' column in our gentrification index isn't here. Let's dig into the data dictionaries of these two datasets to see what's happening here.

'tractid' in Gentrification is described as 'Census tract FIPS code', so it seems the 'CT2010' from the Census is missing the FIPS code. A bit of Googling should reveal what we're missing here.

This guide tells us what the FIPS county code for each borough is: https://guides.newman.baruch.cuny.edu/nyc_data#:~:text=The%20Bronx%20is%20Bronx%20County,County%20(ANSI%20%2F%20FIPS%2036081)

*   The Bronx is Bronx County (ANSI / FIPS 36005)
*   Brooklyn is Kings County (ANSI / FIPS 36047)
*   Manhattan is New York County (ANSI / FIPS 36061)
*   Queens is Queens County (ANSI / FIPS 36081)
*   Staten Island is Richmond County (ANSI / FIPS 36085)

Now, we can create a function that recodes the GEOIDs in the Census data according to boroughs' FIPS codes, so that we can then match it to gentrification.

*Some additional reading on Census coding methodology for the extra-nerdy amongst us:*
* https://s-media.nyc.gov/agencies/dcp/assets/files/pdf/data-tools/bytes/nyct2020_metadata.pdf
* https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html
* Census tract = FIPS(STATE+COUNTY)+TRACT = 2+3+6=11
"""

def recode_GEOID(row):
    if "Manhattan" in row["BoroName"]:
        return "36061" + row["CT2010"]
    elif "Bronx" in row["BoroName"]:
        return "36005" + row["CT2010"]
    elif "Brooklyn" in row["BoroName"]:
        return "36047" + row["CT2010"]
    elif "Queens" in row["BoroName"]:
        return "36081" + row["CT2010"]
    elif "Staten Island" in row["BoroName"]:
        return "36085" + row["CT2010"]
    else:
        return "Invalid BoroCD"

Censustracts["GEOID"] = Censustracts.apply(recode_GEOID, axis=1)

Censustracts['GEOID']=Censustracts['GEOID'].astype(int)

Censustracts

"""
> Amazing! Alright, now let's merge the Census data and the Gentrification index:
"""

joined_gdf = Censustracts.merge(gent_df_reduced, on="GEOID", how='left')

joined_gdf

"""> Great stuff, now let's reduce it back to the variables we're most interested in - as discussed before.



"""

joined_gdf_reduced = joined_gdf[['GEOID',
             'nta_name',
             'NTAName',
             'score_0.5',
             'geometry',
             'BoroName',
             'totpop00'
             ]].copy()

joined_gdf_reduced

"""> Now that we've got the census tracts primed, we need to have a think about how to match the arrest points...

## Joining the data: part 2

### Spatial join of arrest points and the tract shapes

As the arrests data is plotted using points and the census tracts are shapes, we need a way of figuring out in which tract each arrest falls in.

The steps we'll need to take are:

1.   Import all of our tools
2.   Encode the long/lat values in the dataset in a new column and apply the 'point' function to them, then set the coordinate reference system for the points to be the same as the GeoDataFrame from earlier
3.   Run an intersection so any points 'within' the tract shape are assigned to their respective tracts
4.   Tally up the number of arrests per tract, so we have just one row per tract
5.   Merge the summed dataframes as two columns (2006 and 2016) to the GeoDataFrame

This guide really helped me figure out this part: https://www.matecdev.com/posts/point-in-polygon.html#:~:text=for%20further%20analysis.-,How%20to%20check%20if%20a%20point%20is%20inside%20a%20polygon,a%20polygon%20contains%20a%20point.
"""

# Step 1
from shapely import wkt
import matplotlib.pyplot as plt

# Step 2
reduced_df_three_2016['Lon_Lat'] = list(zip(reduced_df_three_2016['Longitude'],df['Latitude']))
reduced_df_three_2016['Lon_Lat'] = reduced_df_three_2016['Lon_Lat'].apply(Point)
points_2016 = gpd.GeoDataFrame(reduced_df_three_2016, geometry='Lon_Lat', crs=joined_gdf_reduced.crs)


reduced_df_three_2006['Lon_Lat'] = list(zip(reduced_df_three_2006['Longitude'],df['Latitude']))
reduced_df_three_2006['Lon_Lat'] = reduced_df_three_2006['Lon_Lat'].apply(Point)
points_2006 = gpd.GeoDataFrame(reduced_df_three_2006, geometry='Lon_Lat', crs=joined_gdf_reduced.crs)

# Step 3
arrests_points_2016 = gpd.tools.sjoin(points_2016, joined_gdf_reduced, predicate="within", how='left')

arrests_points_2006 = gpd.tools.sjoin(points_2006, joined_gdf_reduced, predicate="within", how='left')

# Step 4
censust_tracts_tallied_2016 = arrests_points_2016.groupby(['GEOID'])['GEOID'].size().reset_index(name='Arrests, 2016')

censust_tracts_tallied_2016

censust_tracts_tallied_2006 = arrests_points_2006.groupby(['GEOID'])['GEOID'].size().reset_index(name='Arrests, 2006')

censust_tracts_tallied_2006

# Step 5
joined_gdf_2016 = joined_gdf_reduced.merge(censust_tracts_tallied_2016, on="GEOID", how='left')
joined_gdf_final = joined_gdf_2016.merge(censust_tracts_tallied_2006, on="GEOID", how='left')

joined_gdf_final

"""> Brilliant! Now that we've ***finally*** got all of our data prepped, we can run a few analyses.

## Analysis: part 1

The first thing we should do is normalise the number of arrests by the population. So we'll divide the number of arrests by the population in that census tract, across the two years we have data for (caveat that our population remains fixed to the year 2000).

Then, we can begin mapping the arrests and gentrification index to see if there are any interesting insights/overlaps.
"""

joined_gdf_final["Arrests per 1000 people, 2006"] = (joined_gdf_final["Arrests, 2006"] / joined_gdf_final["totpop00"]) *1000
joined_gdf_final["Arrests per 1000 people, 2016"] = (joined_gdf_final["Arrests, 2016"] / joined_gdf_final["totpop00"]) *1000

# And some rows will have NaN because there were no arrests, so let's make sure these have 0 in them instead of blank
joined_gdf_final['Arrests per 1000 people, 2016'] = joined_gdf_final['Arrests per 1000 people, 2016'].fillna(0)
joined_gdf_final['Arrests per 1000 people, 2006'] = joined_gdf_final['Arrests per 1000 people, 2006'].fillna(0)

joined_gdf_final["% change in arrests, 2006-2016"] = ((joined_gdf_final["Arrests, 2016"] - joined_gdf_final["Arrests, 2006"]) / joined_gdf_final["Arrests, 2006"]) * 100

joined_gdf_final

"""> Ok great, now let's convert our data back into a JSON so we can map it and look at some output. Here are a couple of guides I used to help me with this step:
> *   https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html
> *   https://plotly.com/python/mapbox-county-choropleth/
"""

import json

result = joined_gdf_final.to_json()
parsed = json.loads(result)
json.dumps(parsed, indent=4)

"""> First, let's see if we can re-create the map from the study with our analysis."""

geojson = parsed

fig = px.choropleth_mapbox(joined_gdf_final, geojson=geojson, color="score_0.5",
                           locations="GEOID", featureidkey="properties.GEOID",
                           center = {"lat": 40.730610, "lon": -73.935242},
                           labels={'score_0.5':'Gentrification score'},
                           opacity=0.9,
                           mapbox_style="carto-positron", zoom=9.5)
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

# This is the map from the study:
from IPython.display import HTML
HTML('<img src="https://i.imgur.com/yCq5RlS.jpg">')

fig = px.choropleth_mapbox(joined_gdf_final, geojson=geojson, color="Arrests per 1000 people, 2016",
                           locations="GEOID", featureidkey="properties.GEOID",
                           center = {"lat": 40.730610, "lon": -73.935242},
                           range_color=(0, 400),
                           mapbox_style="carto-positron", zoom=9.5)
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

fig = px.choropleth_mapbox(joined_gdf_final, geojson=geojson, color="Arrests per 1000 people, 2006",
                           locations="GEOID", featureidkey="properties.GEOID",
                           center = {"lat": 40.730610, "lon": -73.935242},
                           range_color=(0, 400),
                           mapbox_style="carto-positron", zoom=9.5)
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""## Analysis: part 2

Very interesting! So there's definitely an overlap between gentrification and a high arrest/pop count across both years in many regions in Brooklyn and lower Manhattan. However, the area of Harlem in northern Manhattan doesn't have this overlap.

Let's plot the gentrification index against a change in arrests to see if there are any trends here.
"""

fig = px.scatter(
    joined_gdf_final,
    x="% change in arrests, 2006-2016",
    y="score_0.5",
    title="% change in arrests vs. gentrification index, 2006-2016",
    trendline="ols",
)

fig.show()

"""> Ok, but what about breaking these down by borough as it seems the story of Brooklyn is quite different to parts of Manhattan?
> Here's a guide that helped me split these up: https://plotly.com/python/line-and-scatter/
"""

fig = px.scatter(joined_gdf_final,
                 x="score_0.5",
                 y="% change in arrests, 2006-2016",
                 facet_col="BoroName",
                 trendline="ols",
                 labels={"score_0.5": "Gentrification index"},
                 title="% change in arrests vs. gentrification index, 2006-2016 (split by borough)",)
fig.show()

"""> OK, so definitely more of a correlation in place like Brooklyn - but still doesn't seem like there's that much of a relationship. Where does this leave us and our hypothesis?

## Conclusions

Although there are some definite overlaps between arrests and gentrification in regions of Brooklyn and Lower Manhattan, this is not a strong enough link. The graphs plotted above shows a **weak** negative correlation between gentrification and arrests/pop in some boroughs, but further analysis would be helpful here.

Given the richness of the datasets we've merged together, there are also a number of additional analyses that can be run. In particular, we can examine changes in arrests across the profile of the perpetrator (age, sex, race) and the type of crime committed.

For this project, however, we can park our analysis at this stage and come back later. To summarise our findings for the original hypothesis: although there is a relationship between gentrification and arrests per population, this relationship does not seem to be strong enough to explain most of the variation. In particular, we aren't accounting for factors like pre-existing socio-economic status which can skew our results.

Thanks a lot for scrolling through all this code, I hope you learnt a few new things!
"""